{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to extract data from Github, we are going to leverage the Github REST API v3, that can be found in this link https://developer.github.com/v3/.\n",
    "In `config.py` file we need to define the following configuration variables, that are going to be accessed by the current notebook:\n",
    "- `GITHUB_USERNAME`\n",
    "- `GITHUB_TOKEN`\n",
    "- `SQL_ALCHEMY_STRING` (only if we want to save our Github results in a relational database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from pandas.io.json import json_normalize\n",
    "from sqlalchemy import create_engine, engine, text, types, MetaData, Table, String\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2 ms\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 999 Âµs\n"
     ]
    }
   ],
   "source": [
    "# function that converts all object columns to strings, in order to store them efficiently into the database\n",
    "def objects_to_strings(table):\n",
    "    measurer = np.vectorize(len)\n",
    "    df_object = table.select_dtypes(include=[object])\n",
    "    string_columns = dict(zip(df_object, measurer(\n",
    "        df_object.values.astype(str)).max(axis=0)))\n",
    "    string_columns = {key: String(length=value) if value > 0 else String(length=1)\n",
    "                      for key, value in string_columns.items() }\n",
    "    return string_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 40 ms\n"
     ]
    }
   ],
   "source": [
    "engine = create_engine(config.SQL_ALCHEMY_STRING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1 ms\n"
     ]
    }
   ],
   "source": [
    "github_api = \"https://api.github.com\"\n",
    "gh_session = requests.Session()\n",
    "gh_session.auth = (config.GITHUB_USERNAME, config.GITHUB_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2 ms\n"
     ]
    }
   ],
   "source": [
    "def branches_of_repo(repo, owner, api):\n",
    "    branches = []\n",
    "    next = True\n",
    "    i = 1\n",
    "    while next == True:\n",
    "        url = api + '/repos/{}/{}/branches?page={}&per_page=100'.format(owner, repo, i)\n",
    "        branch_pg = gh_session.get(url = url)\n",
    "        branch_pg_list = [dict(item, **{'repo_name':'{}'.format(repo)}) for item in branch_pg.json()]    \n",
    "        branch_pg_list = [dict(item, **{'owner':'{}'.format(owner)}) for item in branch_pg_list]\n",
    "        branches = branches + branch_pg_list\n",
    "        if 'Link' in branch_pg.headers:\n",
    "            if 'rel=\"next\"' not in branch_pg.headers['Link']:\n",
    "                next = False\n",
    "        i = i + 1\n",
    "    return branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 872 ms\n"
     ]
    }
   ],
   "source": [
    "branches = json_normalize(branches_of_repo('spark', 'apache', github_api))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## store permanently into an SQL database\n",
    "branches.to_sql(con=engine, name='branches',\n",
    "                 if_exists='replace', dtype=objects_to_strings(branches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 24 ms\n"
     ]
    }
   ],
   "source": [
    "branches.to_csv('data/branches.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2 ms\n"
     ]
    }
   ],
   "source": [
    "def commits_of_repo_github(repo, owner, api):\n",
    "    commits = []\n",
    "    next = True\n",
    "    i = 1\n",
    "    while next == True:\n",
    "        url = api + '/repos/{}/{}/commits?page={}&per_page=100'.format(owner, repo, i)\n",
    "        commit_pg = gh_session.get(url = url)\n",
    "        commit_pg_list = [dict(item, **{'repo_name':'{}'.format(repo)}) for item in commit_pg.json()]    \n",
    "        commit_pg_list = [dict(item, **{'owner':'{}'.format(owner)}) for item in commit_pg_list]\n",
    "        commits = commits + commit_pg_list\n",
    "        if 'Link' in commit_pg.headers:\n",
    "            if 'rel=\"next\"' not in commit_pg.headers['Link']:\n",
    "                next = False\n",
    "        i = i + 1\n",
    "    return commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1 ms\n"
     ]
    }
   ],
   "source": [
    "def create_commits_df(repo, owner, api):\n",
    "    commits_list = commits_of_repo_github(repo, owner, api)\n",
    "    return json_normalize(commits_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 31.2 s\n"
     ]
    }
   ],
   "source": [
    "commits = create_commits_df('spark', 'apache', github_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## store permanently into an SQL database\n",
    "commits.to_sql(con=engine, name='commits',\n",
    "                 if_exists='replace', dtype=objects_to_strings(commits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 393 ms\n"
     ]
    }
   ],
   "source": [
    "commits.to_csv('data/commits.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pull Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2 ms\n"
     ]
    }
   ],
   "source": [
    "def pulls_of_repo(repo, owner, api):\n",
    "    pulls = []\n",
    "    next = True\n",
    "    i = 1\n",
    "    while next == True:\n",
    "        url = api + '/repos/{}/{}/pulls?page={}&per_page=100'.format(owner, repo, i)\n",
    "        pull_pg = gh_session.get(url = url)\n",
    "        pull_pg_list = [dict(item, **{'repo_name':'{}'.format(repo)}) for item in pull_pg.json()]    \n",
    "        pull_pg_list = [dict(item, **{'owner':'{}'.format(owner)}) for item in pull_pg_list]\n",
    "        pulls = pulls + pull_pg_list\n",
    "        if 'Link' in pull_pg.headers:\n",
    "            if 'rel=\"next\"' not in pull_pg.headers['Link']:\n",
    "                next = False\n",
    "        i = i + 1\n",
    "    return pulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.69 s\n"
     ]
    }
   ],
   "source": [
    "pulls = json_normalize(pulls_of_repo('spark', 'apache', github_api))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## store permanently into an SQL database\n",
    "pulls.to_sql(con=engine, name='pulls',\n",
    "                 if_exists='replace', dtype=objects_to_strings(pulls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 30 ms\n"
     ]
    }
   ],
   "source": [
    "pulls.to_csv('data/pulls.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2 ms\n"
     ]
    }
   ],
   "source": [
    "def issues_of_repo(repo, owner, api):\n",
    "    issues = []\n",
    "    next = True\n",
    "    i = 1\n",
    "    while next == True:\n",
    "        url = api + '/repos/{}/{}/issues?page={}&per_page=100'.format(owner, repo, i)\n",
    "        issue_pg = gh_session.get(url = url)\n",
    "        issue_pg_list = [dict(item, **{'repo_name':'{}'.format(repo)}) for item in issue_pg.json()]    \n",
    "        issue_pg_list = [dict(item, **{'owner':'{}'.format(owner)}) for item in issue_pg_list]\n",
    "        issues = issues + issue_pg_list\n",
    "        if 'Link' in issue_pg.headers:\n",
    "            if 'rel=\"next\"' not in issue_pg.headers['Link']:\n",
    "                next = False\n",
    "        i = i + 1\n",
    "    return issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.92 s\n"
     ]
    }
   ],
   "source": [
    "issues = json_normalize(issues_of_repo('spark', 'apache', github_api))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## store permanently into an SQL database\n",
    "issues.to_sql(con=engine, name='issues',\n",
    "                 if_exists='replace', dtype=objects_to_strings(issues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 32 ms\n"
     ]
    }
   ],
   "source": [
    "issues.to_csv('data/issues.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating All Repo Data\n",
    "The following function is used for generating all the previously disscussed data in a single operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.02 ms\n"
     ]
    }
   ],
   "source": [
    "def generate_repo_data(repo, owner, api):\n",
    "    branches = json_normalize(branches_of_repo(repo, owner, api))\n",
    "    commits = create_commits_df(repo, owner, api)\n",
    "    pulls = json_normalize(pulls_of_repo(repo, owner, api))\n",
    "    issues = json_normalize(issues_of_repo(repo, owner, api))\n",
    "    branches.to_csv('data/branches.csv')\n",
    "    commits.to_csv('data/commits.csv')\n",
    "    pulls.to_csv('data/pulls.csv')\n",
    "    issues.to_csv('data/issues.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3min 53s\n"
     ]
    }
   ],
   "source": [
    "generate_repo_data('spark', 'apache', github_api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contribution Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3 ms\n"
     ]
    }
   ],
   "source": [
    "def statistics_of_repo(repo, owner, api):\n",
    "    contributors = []\n",
    "    next = True\n",
    "    i = 1\n",
    "    while next == True:\n",
    "        url = api + '/repos/{}/{}/stats/contributors?page={}&per_page=100'.format(owner, repo, i)\n",
    "        contrib_pg = gh_session.get(url = url)\n",
    "        contrib_pg_list = [dict(item, **{'repo_name':'{}'.format(repo)}) for item in contrib_pg.json()]    \n",
    "        contrib_pg_list = [dict(item, **{'owner':'{}'.format(owner)}) for item in contrib_pg_list]\n",
    "        contributors = contributors + contrib_pg_list\n",
    "        if 'Link' in contrib_pg.headers:\n",
    "            if 'rel=\"next\"' not in contrib_pg.headers['Link']:\n",
    "                next = False\n",
    "        i = i + 1\n",
    "    return contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contribs = statistics_of_repo('spark', 'apache', github_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks_list = []\n",
    "for i in (contrib_list):\n",
    "    for j in i['weeks']:\n",
    "        j['author'] = i['author']['login']\n",
    "weeks_list.append(j)\n",
    "weeks_df = json_normalize(weeks_list)\n",
    "weeks_df['date'] = pd.to_datetime(weeks_df['w'],unit='s')\n",
    "weeks_df['week'] = weeks_df['date'].dt.week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6 ms\n"
     ]
    }
   ],
   "source": [
    "weeks_df.to_sql(con=engine, name='contributions',\n",
    "                 if_exists='replace', dtype=objects_to_strings(weeks_df))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "pygit.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
