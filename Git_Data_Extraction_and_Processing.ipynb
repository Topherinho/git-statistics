{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "FKvEX0r5ujmF",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "b517651b-d21e-425a-98b7-01a5de5e012b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/h44z/pygitea\n",
      "  Cloning https://github.com/h44z/pygitea to c:\\users\\user\\appdata\\local\\temp\\pip-req-build-hi3nea5b\n",
      "Requirement already satisfied (use --upgrade to upgrade): pygitea==0.0 from git+https://github.com/h44z/pygitea in c:\\users\\user\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: parse in c:\\users\\user\\anaconda3\\lib\\site-packages (from pygitea==0.0) (1.12.1)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from pygitea==0.0) (2.22.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->pygitea==0.0) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->pygitea==0.0) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->pygitea==0.0) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->pygitea==0.0) (3.0.4)\n",
      "Building wheels for collected packages: pygitea\n",
      "  Building wheel for pygitea (setup.py): started\n",
      "  Building wheel for pygitea (setup.py): finished with status 'done'\n",
      "  Created wheel for pygitea: filename=pygitea-0.0-cp37-none-any.whl size=4121 sha256=7dc5bd4762e4f5aa17f48ae1de0418451d26b93e73c9d3735e2db3c20d0b539b\n",
      "  Stored in directory: C:\\Users\\User\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-oy2jitr7\\wheels\\29\\c9\\b2\\7e46bec0d76d405d6f586c1baf3e22e12b2b9393e785ed8aa4\n",
      "Successfully built pygitea\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/h44z/pygitea 'C:\\Users\\User\\AppData\\Local\\Temp\\pip-req-build-hi3nea5b'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/h44z/pygitea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygitea\n",
    "import json\n",
    "import requests\n",
    "from pandas.io.json import json_normalize\n",
    "from sqlalchemy import create_engine, engine, text, types, MetaData, Table, String\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "%load_ext autotime\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sqlalchemy import String\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.96 ms\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2 ms\n"
     ]
    }
   ],
   "source": [
    "def objects_to_strings(table):\n",
    "    measurer = np.vectorize(len)\n",
    "    df_object = table.select_dtypes(include=[object])\n",
    "    string_columns = dict(zip(df_object, measurer(\n",
    "        df_object.values.astype(str)).max(axis=0)))\n",
    "    string_columns = {key: String(length=value) if value > 0 else String(length=1)\n",
    "                      for key, value in string_columns.items() }\n",
    "    return string_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 78 ms\n"
     ]
    }
   ],
   "source": [
    "engine = create_engine(config.SQL_ALCHEMY_STRING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gitea APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = pygitea.API(config.GITEA_APP_URL, token=config.GITEA_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 771 ms\n"
     ]
    }
   ],
   "source": [
    "organizations_list = []\n",
    "for i in range(1,3):\n",
    "    organizations_list = organizations_list + api.get('/admin/orgs?page={}&limit=50'.format(i)).json()\n",
    "organizations_df = json_normalize(organizations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.39 s\n"
     ]
    }
   ],
   "source": [
    "organizations_df.to_sql(con=engine, name='organizations',\n",
    "                 if_exists='replace', dtype=objects_to_strings(organizations_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. All Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = api.get('/admin/users')\n",
    "users_df = json_normalize(users.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.to_sql(con=engine, name='users',\n",
    "                 if_exists='replace', dtype=objects_to_strings(users_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. All Repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos = []\n",
    "for index, row in organizations_df.iterrows():\n",
    "    repos = repos + api.get('/orgs/{}/repos'.format(row['username'])).json()\n",
    "\n",
    "for index, row in users_df.iterrows():\n",
    "    repos = repos + api.get('/users/{}/repos'.format(row['login'])).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos_df = json_normalize(repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos_df.to_sql(con=engine, name='repos',\n",
    "                 if_exists='replace', dtype=objects_to_strings(repos_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. All Branches of All Repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 29 s\n"
     ]
    }
   ],
   "source": [
    "branches = []\n",
    "for index, row in repos_df.iterrows():\n",
    "    branches_temp = api.get('/repos/{}/{}/branches'.format(row['owner.login'],row['name'])).json()\n",
    "    branches_temp = [dict(item, **{'repo_name':'{}'.format(row['name'])}) for item in branches_temp]    \n",
    "    branches_temp = [dict(item, **{'owner':'{}'.format(row['owner.login'])}) for item in branches_temp] \n",
    "    branches = branches + branches_temp\n",
    "branches_df = json_normalize(branches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.03 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\pymysql\\cursors.py:170: Warning: (1366, \"Incorrect string value: '\\\\xE2\\\\x80\\\\x8C' for column 'commit.message' at row 330\")\n",
      "  result = self._query(query)\n"
     ]
    }
   ],
   "source": [
    "branches_df.to_sql(con=engine, name='branches',\n",
    "                 if_exists='replace', dtype=objects_to_strings(branches_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Commits of all Repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commits = []\n",
    "# for index, row in repos_df.iterrows():\n",
    "#     first_commit_pg = api.get('/repos/{}/{}/commits'.format(row['owner.login'],row['name']))\n",
    "#     first_commit_pg_list = [dict(item, **{'repo_name':'{}'.format(row['name'])}) for item in first_commit_pg.json()]    \n",
    "#     first_commit_pg_list = [dict(item, **{'repo_full_name':'{}'.format(row['full_name'])}) for item in first_commit_pg_list] \n",
    "#     nr_pages = int(first_commit_pg.headers['X-Pagecount'])\n",
    "#     commits = commits + first_commit_pg_list\n",
    "#     if nr_pages > 1:\n",
    "#         for i in range(2, nr_pages+1):\n",
    "#             commits = commits + api.get('/repos/{}/{}/commits/?page={}'.format(row['owner.login'],row['name'],i)).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. All Commits of an Org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.02 ms\n"
     ]
    }
   ],
   "source": [
    "def repos_of_org(org):\n",
    "    org_repos = repos_df[repos_df['owner.login'] == org]\n",
    "    return org_repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 998 µs\n"
     ]
    }
   ],
   "source": [
    "def get_commits_per_page(owner, repo, page, api):\n",
    "    commit_pg = api.get('/repos/{}/{}/commits/?page={}'.format(owner, repo, page))\n",
    "    commit_pg_list = [dict(item, **{'repo_name':'{}'.format(repo)}) for item in commit_pg.json()]    \n",
    "    commit_pg_list = [dict(item, **{'owner':'{}'.format(owner)}) for item in commit_pg_list]\n",
    "    return commit_pg, commit_pg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.99 ms\n"
     ]
    }
   ],
   "source": [
    "def commits_of_repo(repo, owner, api):\n",
    "    first_commit_pg, first_commit_pg_list = get_commits_per_page(owner, repo, 1, api)\n",
    "    nr_pages = int(first_commit_pg.headers['X-Pagecount'])\n",
    "    commits = []\n",
    "    commits = commits + first_commit_pg_list\n",
    "    if nr_pages > 1:\n",
    "        for i in range(2, nr_pages+1):\n",
    "            commits_page, commits_page_list = get_commits_per_page(owner, repo, i, api)\n",
    "            commits = commits + commits_page_list\n",
    "    return commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.98 ms\n"
     ]
    }
   ],
   "source": [
    "def commits_of_org(org, api):\n",
    "    org_repos = repos_of_org(org)\n",
    "    commits = []\n",
    "    for index, row in org_repos.iterrows():\n",
    "        commits = commits + commits_of_repo(row['name'],row['owner.login'], api)\n",
    "    return commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "org_commits = commits_of_org('Huknow', api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.73 s\n"
     ]
    }
   ],
   "source": [
    "commits_df = json_normalize(org_commits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 28 ms\n"
     ]
    }
   ],
   "source": [
    "del commits_df['parents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.27 s\n"
     ]
    }
   ],
   "source": [
    "commits_df['commit_date'] =  pd.to_datetime(commits_df['commit.committer.date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 37 ms\n"
     ]
    }
   ],
   "source": [
    "commits_df['commit_date'] =  pd.to_datetime(commits_df['commit_date'], utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.02 ms\n"
     ]
    }
   ],
   "source": [
    "commits_df['commit_hour'] = commits_df['commit_date'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.37 s\n"
     ]
    }
   ],
   "source": [
    "commits_df.to_sql(con=engine, name='commits',\n",
    "                 if_exists='replace', dtype=objects_to_strings(commits_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Github API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.98 ms\n"
     ]
    }
   ],
   "source": [
    "github_api = \"https://api.github.com\"\n",
    "gh_session = requests.Session()\n",
    "gh_session.auth = (config.GITHUB_USERNAME, config.GITHUB_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6 ms\n"
     ]
    }
   ],
   "source": [
    "def branches_of_repo(repo, owner, api):\n",
    "    branches = []\n",
    "    next = True\n",
    "    i = 1\n",
    "    while next == True:\n",
    "        url = api + '/repos/{}/{}/branches?page={}&per_page=100'.format(owner, repo, i)\n",
    "        branch_pg = gh_session.get(url = url)\n",
    "        branch_pg_list = [dict(item, **{'repo_name':'{}'.format(repo)}) for item in branch_pg.json()]    \n",
    "        branch_pg_list = [dict(item, **{'owner':'{}'.format(owner)}) for item in branch_pg_list]\n",
    "        branches = branches + branch_pg_list\n",
    "        if 'rel=\"next\"' not in branch_pg.headers['Link']:\n",
    "            next = False\n",
    "        i = i + 1\n",
    "    return branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 872 ms\n"
     ]
    }
   ],
   "source": [
    "superset_branches = json_normalize(branches_of_repo('incubator-superset', 'apache', github_api))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## store permanently into an SQL database\n",
    "superset_branches.to_sql(con=engine, name='superset_branches',\n",
    "                 if_exists='replace', dtype=objects_to_strings(superset_branches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 24 ms\n"
     ]
    }
   ],
   "source": [
    "superset_branches.to_csv('data/branches.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6 ms\n"
     ]
    }
   ],
   "source": [
    "def commits_of_repo_github(repo, owner, api):\n",
    "    commits = []\n",
    "    next = True\n",
    "    i = 1\n",
    "    while next == True:\n",
    "        url = api + '/repos/{}/{}/commits?page={}&per_page=100'.format(owner, repo, i)\n",
    "        commit_pg = gh_session.get(url = url)\n",
    "        commit_pg_list = [dict(item, **{'repo_name':'{}'.format(repo)}) for item in commit_pg.json()]    \n",
    "        commit_pg_list = [dict(item, **{'owner':'{}'.format(owner)}) for item in commit_pg_list]\n",
    "        commits = commits + commit_pg_list\n",
    "        if 'rel=\"next\"' not in commit_pg.headers['Link']:\n",
    "            next = False\n",
    "        i = i + 1\n",
    "    return commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.96 ms\n"
     ]
    }
   ],
   "source": [
    "def create_commits_df(repo, owner, api):\n",
    "    commits_list = commits_of_repo_github(repo, owner, api)\n",
    "    commits_df = json_normalize(commits_list)\n",
    "    del commits_df['parents']\n",
    "    commits_df['commit_date'] =  pd.to_datetime(commits_df['commit.committer.date'])\n",
    "    commits_df['commit_date'] =  pd.to_datetime(commits_df['commit_date'], utc=True)\n",
    "    commits_df['commit_hour'] = commits_df['commit_date'].dt.hour\n",
    "    return commits_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 33.5 s\n"
     ]
    }
   ],
   "source": [
    "superset_df = create_commits_df('incubator-superset', 'apache', github_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## store permanently into an SQL database\n",
    "superset_df.to_sql(con=engine, name='superset_commits',\n",
    "                 if_exists='replace', dtype=objects_to_strings(superset_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 378 ms\n"
     ]
    }
   ],
   "source": [
    "superset_df.to_csv('data/commits.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pull Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.99 ms\n"
     ]
    }
   ],
   "source": [
    "def pulls_of_repo(repo, owner, api):\n",
    "    pulls = []\n",
    "    next = True\n",
    "    i = 1\n",
    "    while next == True:\n",
    "        url = api + '/repos/{}/{}/pulls?page={}&per_page=100'.format(owner, repo, i)\n",
    "        pull_pg = gh_session.get(url = url)\n",
    "        pull_pg_list = [dict(item, **{'repo_name':'{}'.format(repo)}) for item in pull_pg.json()]    \n",
    "        pull_pg_list = [dict(item, **{'owner':'{}'.format(owner)}) for item in pull_pg_list]\n",
    "        pulls = pulls + pull_pg_list\n",
    "        if 'Link' in pull_pg.headers:\n",
    "            if 'rel=\"next\"' not in pull_pg.headers['Link']:\n",
    "                next = False\n",
    "        i = i + 1\n",
    "    return pulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.69 s\n"
     ]
    }
   ],
   "source": [
    "superset_pulls = json_normalize(pulls_of_repo('incubator-superset', 'apache', github_api))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## store permanently into an SQL database\n",
    "superset_pulls.to_sql(con=engine, name='superset_pulls',\n",
    "                 if_exists='replace', dtype=objects_to_strings(superset_pulls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 30 ms\n"
     ]
    }
   ],
   "source": [
    "superset_pulls.to_csv('data/pulls.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.98 ms\n"
     ]
    }
   ],
   "source": [
    "def issues_of_repo(repo, owner, api):\n",
    "    issues = []\n",
    "    next = True\n",
    "    i = 1\n",
    "    while next == True:\n",
    "        url = api + '/repos/{}/{}/issues?page={}&per_page=100'.format(owner, repo, i)\n",
    "        issue_pg = gh_session.get(url = url)\n",
    "        issue_pg_list = [dict(item, **{'repo_name':'{}'.format(repo)}) for item in issue_pg.json()]    \n",
    "        issue_pg_list = [dict(item, **{'owner':'{}'.format(owner)}) for item in issue_pg_list]\n",
    "        issues = issues + issue_pg_list\n",
    "        if 'Link' in issue_pg.headers:\n",
    "            if 'rel=\"next\"' not in issue_pg.headers['Link']:\n",
    "                next = False\n",
    "        i = i + 1\n",
    "    return issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.92 s\n"
     ]
    }
   ],
   "source": [
    "superset_issues = json_normalize(issues_of_repo('incubator-superset', 'apache', github_api))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## store permanently into an SQL database\n",
    "superset_issues.to_sql(con=engine, name='superset_issues',\n",
    "                 if_exists='replace', dtype=objects_to_strings(superset_issues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 32 ms\n"
     ]
    }
   ],
   "source": [
    "superset_issues.to_csv('data/issues.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating All Repo Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1 ms\n"
     ]
    }
   ],
   "source": [
    "def generate_repo_data(repo, owner, api):\n",
    "    branches = json_normalize(branches_of_repo(repo, owner, api))\n",
    "    commits = create_commits_df(repo, owner, api)\n",
    "    pulls = json_normalize(pulls_of_repo(repo, owner, api))\n",
    "    issues = json_normalize(issues_of_repo(repo, owner, api))\n",
    "    branches.to_csv('data/branches.csv')\n",
    "    commits.to_csv('data/commits.csv')\n",
    "    pulls.to_csv('data/pulls.csv')\n",
    "    issues.to_csv('data/issues.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 37 s\n"
     ]
    }
   ],
   "source": [
    "generate_repo_data('incubator-superset', 'apache', github_api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contribution Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3 ms\n"
     ]
    }
   ],
   "source": [
    "def statistics_of_repo(repo, owner, api):\n",
    "    contributors = []\n",
    "    next = True\n",
    "    i = 1\n",
    "    while next == True:\n",
    "        url = api + '/repos/{}/{}/stats/contributors?page={}&per_page=100'.format(owner, repo, i)\n",
    "        contrib_pg = gh_session.get(url = url)\n",
    "        contrib_pg_list = [dict(item, **{'repo_name':'{}'.format(repo)}) for item in contrib_pg.json()]    \n",
    "        contrib_pg_list = [dict(item, **{'owner':'{}'.format(owner)}) for item in contrib_pg_list]\n",
    "        contributors = contributors + contrib_pg_list\n",
    "        if 'Link' in contrib_pg.headers:\n",
    "            if 'rel=\"next\"' not in contrib_pg.headers['Link']:\n",
    "                next = False\n",
    "        i = i + 1\n",
    "    return contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superset_contribs = statistics_of_repo('incubator-superset', 'apache', github_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks_list = []\n",
    "for i in (contrib_list):\n",
    "    for j in i['weeks']:\n",
    "        j['author'] = i['author']['login']\n",
    "weeks_list.append(j)\n",
    "weeks_df = json_normalize(weeks_list)\n",
    "weeks_df['date'] = pd.to_datetime(weeks_df['w'],unit='s')\n",
    "weeks_df['week'] = weeks_df['date'].dt.week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6 ms\n"
     ]
    }
   ],
   "source": [
    "weeks_df.to_sql(con=engine, name='contributions',\n",
    "                 if_exists='replace', dtype=objects_to_strings(weeks_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 143 ms\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "pygit.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
